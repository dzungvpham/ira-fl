{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    "    interaction_mia_fedrec,\n",
    ")\n",
    "from dataset import (\n",
    "    MovieLens,\n",
    "    Yelp,\n",
    ")\n",
    "from more_itertools import grouper\n",
    "from ranker import (\n",
    "    CollaborativeFilteringRecommender,\n",
    "    NeuralCollaborativeFilteringRecommender,\n",
    ")\n",
    "from scipy.stats import ks_2samp\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(id, start, end):\n",
    "    set_seed(id)\n",
    "\n",
    "    data = Yelp()\n",
    "\n",
    "    user_ids = data.get_all_user_ids()\n",
    "    item_ids = data.get_all_item_ids()\n",
    "    user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
    "    item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
    "    num_users = len(user_ids)\n",
    "    num_items = len(item_ids)\n",
    "    embedding_dim = 64\n",
    "    neg_sample_ratio = 4\n",
    "\n",
    "    atk_lr = 1e-01\n",
    "    max_iter = 1000\n",
    "    num_atk = 5\n",
    "\n",
    "    epsilons = [1.0, 10.0, 100.0, math.inf]\n",
    "    delta = 1e-08\n",
    "\n",
    "    metrics = Metrics()\n",
    "\n",
    "    user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "    item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    fcf = CollaborativeFilteringRecommender()\n",
    "    fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [16, 8])\n",
    "\n",
    "    for user_id in tqdm(user_ids[start:end]):\n",
    "        # Set up training data\n",
    "        interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
    "        non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
    "\n",
    "        num_pos = len(interacted_items)\n",
    "        sampled_non_interacted_items = random.sample(\n",
    "            non_interacted_items,\n",
    "            min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
    "        )\n",
    "        num_neg = len(sampled_non_interacted_items)\n",
    "        num_data = num_pos + num_neg\n",
    "\n",
    "        user_embedding = (\n",
    "            user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
    "            .detach()\n",
    "            .view(-1)\n",
    "        )\n",
    "        item_embedding = item_embeddings(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
    "                    torch.LongTensor(\n",
    "                        [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ).detach()\n",
    "        interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)])\n",
    "        random_user_emb = torch.rand(embedding_dim)\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            # FCF Simple\n",
    "            target = fcf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = apply_gaussian_mechanism(target, epsilon, delta, sensitivity=100)\n",
    "\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fcf.item_grad(U, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # # FNCF setup\n",
    "            target = fncf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = apply_gaussian_mechanism(target, epsilon, delta, sensitivity=0.005)\n",
    "\n",
    "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "\n",
    "            # FNCF simple\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fncf.item_grad(random_user_emb, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FNCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fncf.item_grad(U, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # FNCF jointly estimate user embedding with neural net params\n",
    "            feature_grad = fncf.feature_grad(user_embedding, item_embedding, interactions)\n",
    "            feature_grad = apply_gaussian_mechanism(feature_grad, epsilon, delta, sensitivity=0.7)\n",
    "\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: (\n",
    "                    fncf.item_grad(U, item_embedding, I),\n",
    "                    fncf.feature_grad(U, item_embedding, I, retain_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # FNCF simple with neural net params\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: (\n",
    "                    fncf.item_grad(random_user_emb, item_embedding, I),\n",
    "                    fncf.feature_grad(random_user_emb, item_embedding, I, retain_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # Random guess\n",
    "            preds_raw = torch.rand(num_data)\n",
    "            metrics.update(\n",
    "                f\"Random_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds_raw.round().long(),\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # IMIA FCF\n",
    "            # target = fcf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            # preds = interaction_mia_fedrec(\n",
    "            #     lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "            #     target,\n",
    "            #     num_data,\n",
    "            #     select_ratio=interactions.mean(),\n",
    "            # )\n",
    "\n",
    "            # metrics.update(\n",
    "            #     \"FCF_IMIA_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            # )\n",
    "\n",
    "    metrics.save(f\"../output/output_part_{id}.csv\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_processes = cpu_count()\n",
    "    pool = Pool()\n",
    "\n",
    "    data = Yelp()\n",
    "    num_users = len(data.get_all_user_ids())\n",
    "    num_users_per_process = math.ceil(num_users / num_processes)\n",
    "\n",
    "    parameters = [\n",
    "        (i, num_users_per_process * i, min(num_users, num_users_per_process * (i + 1)))\n",
    "        for i in range(num_processes)\n",
    "    ]\n",
    "\n",
    "    metrics = pool.starmap(worker, parameters)\n",
    "    final_metrics_df = pd.concat([m.df for m in metrics])\n",
    "    final_metrics_df.to_csv(f\"../output/output_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
